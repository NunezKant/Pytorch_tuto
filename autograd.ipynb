{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Automatic differentation and training loop"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Why autograd is important and how it works: [Why Jacobians are useful to compute gradients in a NN?](https://suzyahyah.github.io/calculus/machine%20learning/2018/04/04/Jacobian-and-Backpropagation.html)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We specify that our tensor requires gradient, i.e, we want to compute a derivate w.r.t. that tensor:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "x = torch.rand(4,requires_grad=True)\n",
    "print(x)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([9.5146e-01, 7.6532e-05, 6.8714e-01, 1.9988e-01], requires_grad=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets start doing some computations and see how pytorch keeps track of those in a [computational graph.](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "y = x*5\n",
    "print(y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([4.7573e+00, 3.8266e-04, 3.4357e+00, 9.9938e-01],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "w = y**2\n",
    "print(w)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([2.2632e+01, 1.4643e-07, 1.1804e+01, 9.9876e-01],\n",
      "       grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "z = w.mean()\n",
    "print(z)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(8.8587, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, each output coming from a computation has an associated `grad_fun` that describes the graph that relates every computation with the initial variable.\n",
    "\n",
    "so if we want to compute $\\frac{\\partial z}{\\partial x}$ we simply do:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "z.backward()\n",
    "print(x.grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([1.1893e+01, 9.5665e-04, 8.5893e+00, 2.4984e+00])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "`z.backward()` produces a *vector-jacobian product* (i.e, chain-rule) and the gradients are stored in the attribute `.grad` of the variable of interest.\n",
    "\n",
    "## How to prevent to compute the gradients ?? \n",
    "\n",
    "There are 3 main ways:\n",
    "\n",
    "| Syntax      | Description |\n",
    "| ----------- | ----------- |\n",
    "| `with torch.no_grad():`  | Context manager that allows us to do computations without gradients|\n",
    "|  `x.requires_grad_(False)` | Sets the requires_grad argument to False in place|\n",
    "| `x.detach()`  | Creates a new tensor without gradient capabilities|\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "x = torch.rand(5,requires_grad=True)\n",
    "print(x)\n",
    "x.requires_grad_(False)\n",
    "print(x)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0.9272, 0.4491, 0.1590, 0.3386, 0.3007], requires_grad=True)\n",
      "tensor([0.9272, 0.4491, 0.1590, 0.3386, 0.3007])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "x = torch.rand(5,requires_grad=True)\n",
    "print(x)\n",
    "y = x.detach()\n",
    "print(y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0.0322, 0.0197, 0.5358, 0.2156, 0.6406], requires_grad=True)\n",
      "tensor([0.0322, 0.0197, 0.5358, 0.2156, 0.6406])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "with torch.no_grad():\n",
    "    y=x**2+2\n",
    "    print(y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([2.0010, 2.0004, 2.2871, 2.0465, 2.4103])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One thing that we should be careful with is that the attribute `.grad` sums up all the gradients, so in many cases we will need to set gradients to zero after a training iteration.\n",
    "\n",
    "Dummy example:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "W = torch.ones(3, requires_grad=True)\n",
    "\n",
    "for epoch in range(4):\n",
    "    out = (W*2).sum()\n",
    "\n",
    "    out.backward()\n",
    "\n",
    "    print(W.grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([2., 2., 2.])\n",
      "tensor([4., 4., 4.])\n",
      "tensor([6., 6., 6.])\n",
      "tensor([8., 8., 8.])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see at each iteration we are storing the previous values of the gradient, how to solve this? "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "W = torch.ones(3, requires_grad=True)\n",
    "\n",
    "for epoch in range(4):\n",
    "    out = (W*2).sum()\n",
    "\n",
    "    out.backward()\n",
    "\n",
    "    print(W.grad)\n",
    "\n",
    "    W.grad.zero_()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([2., 2., 2.])\n",
      "tensor([2., 2., 2.])\n",
      "tensor([2., 2., 2.])\n",
      "tensor([2., 2., 2.])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here `.grad.zero_()` is setting inplace the gradients to zero at the end of each epoch, later we will talk about optimizers and how those have a built-in function that does exactly the same.\n",
    "\n",
    "## Dummy example: Perceptron with one input, one neuron and one output for one epoch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "input = torch.tensor(2.0)\n",
    "output = torch.tensor(10.0)\n",
    "\n",
    "W = torch.tensor(1.0, requires_grad=True) # this requires grad since it is our learnable param\n",
    "pred = W*input\n",
    "loss = (pred - output)**2 #MSE loss\n",
    "print(loss)\n",
    "loss.backward() # backprop"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(64., grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Try to compute the backprop by hand and compare it with the result!!\n",
    "\n",
    "## Pytorch Trainig Loop\n",
    "\n",
    "The basic pytorch model pipeline is given by:\n",
    "\n",
    " 1. Defining the model\n",
    " 2. Defining the loss function and the optimizer\n",
    " 3. Train the model:\n",
    "    1. Compute the prediction: Forward pass\n",
    "    2. Compute the gradients: Backward pass\n",
    "    3. Parameter update\n",
    "    "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "import torch.nn as nn "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "x = torch.randint(1,10,(5,), dtype=torch.float32)\n",
    "y = torch.randint(1,10,(5,), dtype=torch.float32)\n",
    "w = torch.zeros(1, requires_grad=True, dtype=torch.float32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Define the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "def forward(x):\n",
    "    return w*x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Define the loss and optimizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "lr = 0.01\n",
    "n_iter = 11"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w], lr = lr)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Training loop"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "for epoch in range(n_iter):\n",
    "    pred = forward(x)\n",
    "    l = loss(y, pred)\n",
    "    l.backward() # gradients\n",
    "    optimizer.step() #update weights\n",
    "    optimizer.zero_grad() # set grads to 0\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'epoch {epoch}: w = {w}, loss = {l}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0: w = tensor([0.7400], requires_grad=True), loss = 38.599998474121094\n",
      "epoch 5: w = tensor([0.9997], requires_grad=True), loss = 1.6000522375106812\n",
      "epoch 10: w = tensor([1.0000], requires_grad=True), loss = 1.600000023841858\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see how our loss decreases with epochs, and our weight is adjusted."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('Pytorch_Intro-tTLkiOo3': pipenv)"
  },
  "interpreter": {
   "hash": "2dcc0e8861c3ac3e9ce0aee99191a4c13ff6b8d1fd5b0c82dfaa231f15b68aa4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}