{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Automatic differentation and gradient descend "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Why autograd is important and how it works: [Why Jacobians are useful to compute gradients in a NN?](https://suzyahyah.github.io/calculus/machine%20learning/2018/04/04/Jacobian-and-Backpropagation.html)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We specify that our tensor requires gradient, i.e, we want to compute a derivate w.r.t. that tensor:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "x = torch.rand(4,requires_grad=True)\n",
    "print(x)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0.9014, 0.1748, 0.5335, 0.7465], requires_grad=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets start doing some computations and see how pytorch keeps track of those in a [computational graph.](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "y = x*5\n",
    "print(y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([4.5071, 0.8739, 2.6675, 3.7325], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "w = y**2\n",
    "print(w)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([20.3136,  0.7637,  7.1155, 13.9315], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "z = w.mean()\n",
    "print(z)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(10.5311, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, each output coming from a computation has an associated `grad_fun` that describes the graph that relates every computation with the initial variable.\n",
    "\n",
    "so if we want to compute $\\frac{\\partial z}{\\partial x}$ we simply do:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "z.backward()\n",
    "print(x.grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([11.2676,  2.1848,  6.6687,  9.3312])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "`z.backward()` produces a *vector-jacobian product* (i.e, chain-rule) and the gradients are stored in the attribute `.grad` of the variable of interest.\n",
    "\n",
    "## How to prevent to compute the gradients ?? \n",
    "\n",
    "There are 3 main ways:\n",
    "\n",
    "| Syntax      | Description |\n",
    "| ----------- | ----------- |\n",
    "| `with torch.no_grad():`  | Context manager that allows us to do computations without gradients|\n",
    "|  `x.requires_grad_(False)` | Sets the requires_grad argument to False in place|\n",
    "| `x.detach()`  | Creates a new tensor without gradient capabilities|\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "x = torch.rand(5,requires_grad=True)\n",
    "print(x)\n",
    "x.requires_grad_(False)\n",
    "print(x)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0.0090, 0.0596, 0.9173, 0.0520, 0.7266], requires_grad=True)\n",
      "tensor([0.0090, 0.0596, 0.9173, 0.0520, 0.7266])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "x = torch.rand(5,requires_grad=True)\n",
    "print(x)\n",
    "y = x.detach()\n",
    "print(y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0.7471, 0.1520, 0.9719, 0.5339, 0.8926], requires_grad=True)\n",
      "tensor([0.7471, 0.1520, 0.9719, 0.5339, 0.8926])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "with torch.no_grad():\n",
    "    y=x**2+2\n",
    "    print(y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([2.5581, 2.0231, 2.9447, 2.2851, 2.7967])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One thing that we should be careful with is that the attribute `.grad` sums up all the gradients, so in many cases we will need to set gradients to zero after a training iteration.\n",
    "\n",
    "Dummy example:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "W = torch.ones(3, requires_grad=True)\n",
    "\n",
    "for epoch in range(4):\n",
    "    out = (W*2).sum()\n",
    "\n",
    "    out.backward()\n",
    "\n",
    "    print(W.grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([2., 2., 2.])\n",
      "tensor([4., 4., 4.])\n",
      "tensor([6., 6., 6.])\n",
      "tensor([8., 8., 8.])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see at each iteration we are storing the previous values of the gradient, how to solve this? "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "W = torch.ones(3, requires_grad=True)\n",
    "\n",
    "for epoch in range(4):\n",
    "    out = (W*2).sum()\n",
    "\n",
    "    out.backward()\n",
    "\n",
    "    print(W.grad)\n",
    "\n",
    "    W.grad.zero_()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([2., 2., 2.])\n",
      "tensor([2., 2., 2.])\n",
      "tensor([2., 2., 2.])\n",
      "tensor([2., 2., 2.])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here `.grad.zero_()` is setting inplace the gradients to zero at the end of each epoch, later we will talk about optimizers and how those have a built-in function that does exactly the same.\n",
    "\n",
    "## Dummy example: Perceptron with one input, one neuron and one output for one epoch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "input = torch.tensor(2.0)\n",
    "output = torch.tensor(10.0)\n",
    "\n",
    "W = torch.tensor(1.0, requires_grad=True) # this requires grad since it is our learnable param\n",
    "pred = W*input\n",
    "loss = (pred - output)**2 #MSE loss\n",
    "print(loss)\n",
    "loss.backward() # backprop"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(64., grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Try to compute the backprop by hand and compare it with the result! "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('Pytorch_Intro-tTLkiOo3': pipenv)"
  },
  "interpreter": {
   "hash": "2dcc0e8861c3ac3e9ce0aee99191a4c13ff6b8d1fd5b0c82dfaa231f15b68aa4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}